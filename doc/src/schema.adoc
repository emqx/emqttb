:!sectids:
:stem:
= Documentation

[id=cluster.node_name]
== Node name

Note: erlang distribution is disabled when node name is `undefined`.

[id=restapi.enabled]
== Enable REST API
`+--restapi+` CLI argument enables REST API (by default it's available at http://127.0.0.0:8017), and it also means that the script keeps running after completing the scenarios.

[id=autorate]
== Autorate configuration

When the loadgen creates too much traffic, the system may get overloaded.
In this case, the test usually has to be restarted all over again with different parameters.
This can be very expensive in man-hours and computing resources.

In order to prevent that, emqttb can tune some parameters (such as message publishing interval)
automatically using https://controlguru.com/integral-reset-windup-jacketing-logic-and-the-velocity-pi-form/[PI controller].

The following formula is used for the error function:

stem:[e=(a_{SP} - a_{PV}) a_{coeff}]

=== Autoscale

A special autorate controlling the rate of spawning new clients is implicitly created for each client group.
Its name usually follows the pattern `%scenario%/conn_interval`.


By default, the number of pending (unacked) connections is used as the process variable.
Number of pending connections is a metric that responds very fast to target overload, so it makes a reasonable default.

For example the following command can automatically adjust the rate of connections:

[code,bash]
----
./emqttb --pushgw @conn -I 10ms -N 5_000 \
         @a -a conn/conninterval -V 1000 --setpoint 10
----


[id=autorate._.id]
== ID of the autorate configuration

Autorate configuration can be referred by id.
This value must be equal to one of the elements returned by `emqttb @ls autorate` command.
Full list is also available in <<autorate>>


[id=autorate._.speed]
== Maximum rate of change of the controlled parameter

Note: by default this parameter is set to 0 for each autorate, effectively locking the control parameter in place.


[id=autorate._.process_variable]
== Process variable

This parameter specifies ID of the metric that senses pressure on the SUT and serves as the process variable (PV).
Its value must be equal to one of the metric IDs returned by `emqttb @ls metric` command.
Full list can be also found in <<metrics>>.

[id=autorate._.setpoint]
== Setpoint

The desired value of the process variable (PV) is called the setpoint.
Autorate adjusts the value of the control variable (CV) to bring the PV close to the setpoint.


[id=interval]
== Default interval between events

Supported units:

* `us`: microseconds
* `ms`: milliseconds
* `s`: seconds
* `min`: minutes
* `h` : hours

If unit is not specified then `ms` is assumed.

[id=scenarios.sub]
== Run scenario sub

This scenario starts `-N` workers, which subscribe to a specified topic.
The only mandatory parameter is `--topic`, which supports pattern substitutions.

[id=scenarios.sub._.clean_start]
== Clean start
Note: in order to disable clean start (and make the session persistent) this flag should be set to `false` (for example, `./emqttb @sub +c ...` via CLI).

[id=scenarios.sub._.parse_metadata]
== Extract metadata from message payloads

Subscribers will report end-to-end latency when this option is enabled.

WARNING: Publishers should insert metadata into the payloads.
For example, when using <<value.scenarios.pub,pub scenario>> it's necessary to enable <<value.scenarios.pub._.metadata,metadata>> generation.

WARNING: In order to measure latency accurately, the scenario should ensure that publishers reside on the same emqttb host with the subscribers.
Otherwise clock skew between different load generator instances will introduce a systematic error.


[id=scenarios.sub._.verify_sequence]
== Verify sequence of messages

When this option is enabled, emqttb will parse the metadata embedded in the messages and check for missing or duplicated messages.
This option implies <<value.scenarios.sub._.parse_metadata>>.

Errors about missing messages and warnings about duplicate messages are printed to the `emqttb.log`.
Relevant prometheus metrics include:

- `emqttb_repeats_number` -- number of times when the sequence number of the message goes backwards
- `emqttb_gaps_number` -- number of times when the sequence number of the message skips the messages (a gap)
- `emqttb_repeat_size` -- rolling average; size of the repeated sequence
- `emqttb_gap_size` -- rolling average; size of the gap


WARNING: Publishers should insert metadata into the payloads in order for this feature to work.

WARNING: This feature can use a lot of RAM to store the sequence numbers for each triple of sender client id, receiver client id, and MQTT topic.

=== Client groups

- `sub`

[id=scenarios.conn]
== Run scenario conn

This scenario starts `-N` workers, which connect to the broker and then simply linger around.

=== Client groups

- `conn`


[id=scenarios.pub]
== Run scenario pub

This scenario starts `-N` workers, which publish messages to the specified topic at period `--pubinterval`.
The only mandatory parameter is `--topic`, which supports pattern substitutions.

=== Client groups

- `pub`

=== Examples
==== Basic usage

[code,bash]
----
emqttb @pub -t foo/%n -N 100 -i 10ms -s 1kb
----

In this example the loadgen connects to the default broker <link xlink:href="mqtt://localhost:1883"/>,
starts 100 publishers which send messages to topic with the suffix of the worker id every 10 milliseconds. Size of the messages is 1kb.

==== Changing client settings

[code,bash]
----
emqttb @pub -t foo/%n @g --ssl --transport ws -h 127.0.0.1
----

In this example settings of the default client group has been changed: TLS encryption is enabled, and WebSocket transport is used.
Also the hostname of the broker is specified explicitly.

[code,bash]
----
emqttb @pub -t foo/%n -q 1 -g pub @g -g pub --ssl --transport ws -h 127.0.0.1
----

The below example is similar to the previous one, except QoS of the messages is set to 1,
and a dedicated client configuration with id `pub` is used for the publishers.
It's useful for running multiple scenarios (e.g. `@pub` and `@sub`) in parallel, when they must use
different settings. For example, it can be used for testing MQTT bridge.


==== Tuning publishing rate automatically

By default, `@pub` scenario keeps `pubinterval` constant.
However, in some situations it should be tuned dynamically: suppose one wants to measure what publishing rate the broker can sustain while keeping publish latency under `--publatency`.

This is also useful for preventing system overload.
Generating too much load can bring the system down, and the test has to be started all over again with different parameters.
Sometimes finding the correct rate takes many attempts, wasting human and machine time.
Dynamic tuning of the publishing rate for keeping the latency constant can help in this situation.

By default the maximum speed of rate adjustment is set to 0, effectively locking the `pubinterval` at a constant value.
To enable automatic tuning, the autorate speed `-V` must be set to a non-zero value, also it makes sense to set
the minimum (`-m`) and maximum (`-M`) values of the autorate:

[code,bash]
----
emqttb @pub -t foo -i 1s -q 1 --publatency 50ms @a -V 10 -m 0 -M 10000
----

Once automatic adjustment of the publishing interval is enabled, `-i` parameter sets the starting value of the publish interval,
rather than the constant value. So the above example reads like this:

Publish messages to topic `foo` with QoS 1, starting at the publishing interval of 1000 milliseconds, dynamically adjusting it
so to keep the publishing latency around 50 milliseconds. The publishing interval is kept between 0 and 10 seconds,
and the maximum rate of its change is 10 milliseconds per second.

=== Client groups
- `pub`

[id=scenarios.pub._.topic]
== Topic where the clients shall publish messages

Topic is a mandatory parameter. It supports the following substitutions:

* `%n` is replaced with the worker ID (integer)
* `%g` is replaced with the group ID
* `%h` is replaced with the hostname


[id=scenarios.pub._.clean_start]
== Clean start
Set https://docs.oasis-open.org/mqtt/mqtt/v5.0/os/mqtt-v5.0-os.html#_Toc3901039[clean start] flag in the CONNECT packet.

[id=scenarios.pub._.expiry]
== Session Expiry Interval
Add https://docs.oasis-open.org/mqtt/mqtt/v5.0/os/mqtt-v5.0-os.html#_Toc3901048[Session Expiry Interval] property to the CONNECT packet.


[id=scenarios.pubsub_forward]
== run scenario pubsub_forward

First all subscribers connect and subscribe to the brokers, then the
publishers start to connect and publish.  The default is to use full
forwarding of messages between the nodes: that is, each publisher
client publishes to a topic subscribed by a single client, and both
clients reside on distinct nodes.

Full forwarding of messages is the default and can be set by full_forwarding.

=== Examples
==== Basic usage

[code,bash]
----
./emqttb --restapi @pubsub_fwd --publatency 10ms --num-clients 400 -i 70ms \
                   @g -h 172.25.0.2:1883,172.25.0.3:1883,172.25.0.4:1883
----

In this example the loadgen connects to a list of brokers
in a round-robin in the declared order.  First all the
subscribers, then the publishers, with the difference that
publishers will shift the given host list by one position
to ensure each publisher and subscriber pair will reside
on different hosts, thus forcing all messages to be
forwarded.

=== Client groups

- `pubsub_forward.pub`
- `pubsub_forward.sub`

[id=scenarios.persistent_session]

== Run scenario persistent_session

This scenario measures throughput of MQTT broker in presence of persistent sessions.
It is split in two stages that repeat in a loop:

- `consume` stage where subscribers (re)connect to the broker with `clean_session=false` and ingest saved messages
- `publish` stage where subscribers disconnect, and another group of clients publishes messages to the topics

This separation helps to measure throughput of writing and reading messages independently.

Publish stage runs for a <<value.scenarios.persistent_session._.pub.pub_time,set period of time>>.
It's possible to adjust publishing rate via autorate.

Consume stages runs until the subscribers ingest all published messages,
or until <<value.scenarios.persistent_session._.max_stuck_time,timeout>>.
Please note that throughput measurement is not reliable when the consume stage is aborted due to timeout.

=== Examples

=== Client groups

- `persistent_session.pub`
- `persistent_session.sub`

[id=scenarios.persistent_session._.pub.qos]
== QoS of the published messages

WARNING: changing QoS to any value other then 2 is likely to cause consume stage to hang,
since it has to consume the exact number of messages as previously produced.

[id=scenarios.persistent_session._.sub.qos]
== Subscription QoS

WARNING: changing QoS to any value other then 2 is likely to cause consume stage to hang,
since it has to consume the exact number of messages as previously produced.

[id=groups]
== Configuration for client groups
Client configuration is kept separate from the scenario config.
This is done so scenarios could share client configuration.

[id=groups._.net.ifaddr]
== Local IP addresses

Bind a specific local IP address to the connection.
If multiple IP addresses are given, workers choose local address using round-robin algorithm.

WARNING: Setting a local address for a client TCP connection explicitly has a nasty side effect:
when you do this `gen_tpc` calls `bind` on this address to get a free ephemeral port.
But the OS doesn't know that in advance that we won't be listening on the port, so it reserves the local port number for the connection.
However, when we connect to multiple EMQX brokers, we do want to reuse local ports.
So don't use this option when the number of local addresses is less than the number of remote addresses.


[id=groups._.client.clientid]
== Clientid pattern

Pattern used to generate ClientID.
The following substitutions are supported:

* `%n` is replaced with the worker ID (integer)
* `%g` is replaced with the group ID
* `%h` is replaced with the hostname


[id=groups._.connection.keepalive]
== Keepalive time

How often the clients will send `PING` MQTT message to the broker on idle connections.

[id=autorate._.scram.enabled]
== SCRAM
Normally, autorate adjusts the control variable gradually.
However, sometimes the system under test becomes overloaded suddenly, and in this case slowly decreasing the pressure may not be efficient enough.
To combat this situation, `emqttb` has "SCRAM" mechanism, that immediately resets the control variable to a <<value.autorate.$$_$$.scram.override,configured safe value>>.
This happens when the value of process variable exceeds the <<value.autorate.$$_$$.scram.threshold,threshold>>.

SCRAM mode remains in effect until the number of pending connections becomes less than
_threshold_ * <<value.autorate.$$_$$.scram.hysteresis,hystersis>> / 100.


[id=autorate._.scram.threshold]
== SCRAM threshold


[id=autorate._.scram.override]
== SCRAM rate override
Replace configured (or calculated via autorate) value of the control variable with this value when the system under test is not keeping up with the load.


[id=autorate._.scram.hysteresis]
== SCRAM hysteresis
It's not desirable to switch between normal and SCRAM connection rate too often.

[id=autorate._.update_interval]
== How often autorate is updated

This parameter governs how often error is calculated and control parameter is updated.
